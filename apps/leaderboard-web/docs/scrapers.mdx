---
title: Scrapers
description: Collect contribution data from various platforms
---

# Scrapers

Learn how to collect contribution data from various platforms using scrapers.

## Overview

Scrapers collect contribution data from various platforms and transform it into the format expected by the leaderboard. The leaderboard supports multiple scrapers that can run independently or together.

## GitHub Scraper

Tracks GitHub activities across your organization's repositories.

### What It Tracks

- **Pull Requests**: Opened, merged, reviewed, commented
- **Issues**: Opened, closed, assigned, commented
- **Code Reviews**: Reviews submitted, comments made
- **Discussions**: Started, participated in, answered

### Setup

1. **Install the scraper:**
   ```bash
   git clone https://github.com/ohcnetwork/leaderboard-github-scraper
   cd leaderboard-github-scraper
   pnpm install
   ```

2. **Configure environment variables:**
   ```bash
   GITHUB_TOKEN=your_github_token
   GITHUB_ORG=your_organization
   OUTPUT_PATH=../leaderboard-data
   ```

3. **Run the scraper:**
   ```bash
   pnpm start
   ```

### GitHub Token

Create a GitHub Personal Access Token with these scopes:
- `repo` - Access to repositories
- `read:org` - Read organization data
- `read:user` - Read user profiles

### Configuration

Configure in `config.yaml`:

```yaml
scraper:
  scrapers:
    - name: GitHub Scraper
      repository: ohcnetwork/leaderboard-github-scraper
      envs:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        GITHUB_ORG: ${{ github.repository_owner }}
```

## Slack Scraper

Monitors Slack engagement and participation.

### What It Tracks

- **Messages**: Messages sent in channels
- **Threads**: Thread participation and replies
- **Reactions**: Reactions given and received
- **Channel Activity**: Active channels and participation

### Setup

1. **Install the scraper:**
   ```bash
   git clone https://github.com/ohcnetwork/leaderboard-slack-scraper
   cd leaderboard-slack-scraper
   pnpm install
   ```

2. **Configure environment variables:**
   ```bash
   SLACK_API_TOKEN=your_slack_token
   SLACK_CHANNELS=channel1,channel2
   OUTPUT_PATH=../leaderboard-data
   ```

3. **Run the scraper:**
   ```bash
   pnpm start
   ```

### Slack Token

Create a Slack App with these scopes:
- `channels:history` - Read public channel messages
- `channels:read` - List public channels
- `users:read` - Read user profiles
- `reactions:read` - Read reactions

### Configuration

Configure in `config.yaml`:

```yaml
scraper:
  scrapers:
    - name: Slack Scraper
      repository: ohcnetwork/leaderboard-slack-scraper
      envs:
        SLACK_API_TOKEN: ${{ secrets.SLACK_API_TOKEN }}
        SLACK_CHANNEL: ${{ vars.SLACK_EOD_CHANNEL }}
```

## Custom Scrapers

Create custom scrapers for other platforms or organization-specific tracking.

### Scraper Requirements

A scraper should:

1. **Fetch data** from the source platform
2. **Transform data** to the leaderboard schema
3. **Output JSON** files in the expected format
4. **Handle errors** gracefully
5. **Support incremental updates** (optional but recommended)

### Output Format

Scrapers should output three JSON files:

**contributors.json:**
```json
[
  {
    "username": "johndoe",
    "name": "John Doe",
    "avatar_url": "https://example.com/avatar.jpg",
    "role": "core",
    "socials": {
      "github": "https://github.com/johndoe"
    }
  }
]
```

**activities.json:**
```json
[
  {
    "id": "unique-id",
    "username": "johndoe",
    "activity_type": "pr_merged",
    "timestamp": "2025-01-15T10:30:00Z",
    "points": 10,
    "metadata": {
      "repository": "org/repo",
      "pr_number": 123
    }
  }
]
```

**aggregates.json:**
```json
[
  {
    "type": "global",
    "key": "total_activities",
    "value": 1234,
    "updated_at": "2025-01-15T10:30:00Z"
  }
]
```

### Example Custom Scraper

Here's a basic structure for a custom scraper:

```typescript
import { writeFileSync } from 'fs';

interface Contributor {
  username: string;
  name: string;
  avatar_url: string;
  role: string;
  socials?: Record<string, string>;
}

interface Activity {
  id: string;
  username: string;
  activity_type: string;
  timestamp: string;
  points: number;
  metadata?: Record<string, any>;
}

async function scrape() {
  const contributors: Contributor[] = [];
  const activities: Activity[] = [];
  
  // Fetch data from your platform
  const data = await fetchFromPlatform();
  
  // Transform to leaderboard format
  for (const item of data) {
    contributors.push({
      username: item.user.username,
      name: item.user.name,
      avatar_url: item.user.avatar,
      role: 'contributor',
    });
    
    activities.push({
      id: item.id,
      username: item.user.username,
      activity_type: 'custom_activity',
      timestamp: item.created_at,
      points: calculatePoints(item),
      metadata: {
        title: item.title,
        url: item.url,
      },
    });
  }
  
  // Write output files
  writeFileSync('contributors.json', JSON.stringify(contributors, null, 2));
  writeFileSync('activities.json', JSON.stringify(activities, null, 2));
}

scrape().catch(console.error);
```

## Automation

### GitHub Actions

Generate a workflow to run scrapers automatically:

```bash
pnpm generate:workflow
```

This creates `.github/workflows/update-data.yml`:

```yaml
name: Update Leaderboard Data

on:
  schedule:
    - cron: '0 */12 * * *'  # Every 12 hours
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - uses: pnpm/action-setup@v2
      - uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: 'pnpm'
      
      - run: pnpm install
      
      # Run scrapers
      - name: Run GitHub Scraper
        run: pnpm scrape:github
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Run Slack Scraper
        run: pnpm scrape:slack
        env:
          SLACK_TOKEN: ${{ secrets.SLACK_TOKEN }}
      
      # Import and deploy
      - run: pnpm db:import
      - run: pnpm build
      
      # Deploy to your platform
      - name: Deploy
        run: pnpm deploy
```

### Cron Schedule

Common cron schedules:

- `0 * * * *` - Every hour
- `0 */6 * * *` - Every 6 hours
- `0 */12 * * *` - Every 12 hours
- `0 0 * * *` - Daily at midnight
- `0 0 * * 0` - Weekly on Sunday

### Manual Triggers

Run scrapers manually:

```bash
# Via GitHub Actions UI
# Go to Actions → Update Leaderboard Data → Run workflow

# Or via CLI
gh workflow run update-data.yml
```

## Data Merging

When multiple scrapers run, their data is merged:

### Contributor Merging

Contributors are matched by `username`. Data is merged with priority:
1. Most recent data
2. Most complete data
3. Explicit role assignments

### Activity Deduplication

Activities are deduplicated by `id`. Ensure your scraper generates unique IDs:

```typescript
const id = `${platform}-${activity_type}-${resource_id}`;
// Example: "github-pr_merged-123"
```

### Aggregate Calculation

Aggregates are recalculated after all scrapers run, combining data from all sources.

## Best Practices

### Rate Limiting

Respect API rate limits:
- Use authentication tokens
- Implement exponential backoff
- Cache responses when possible
- Batch requests

### Incremental Updates

Only fetch new data since last run:
- Store last update timestamp
- Use API pagination
- Filter by date range
- Skip unchanged data

### Error Handling

Handle errors gracefully:
- Retry transient failures
- Log errors for debugging
- Continue on partial failures
- Validate output before writing

### Performance

Optimize scraper performance:
- Parallelize independent requests
- Use connection pooling
- Compress large responses
- Stream large datasets

## Monitoring

### Scraper Logs

Check scraper logs in GitHub Actions:
1. Go to Actions tab
2. Select workflow run
3. View logs for each step

### Data Quality

Validate scraped data:
- Check for missing fields
- Verify data types
- Ensure unique IDs
- Validate timestamps

### Alerts

Set up alerts for scraper failures:
- GitHub Actions notifications
- Slack/Discord webhooks
- Email notifications
- Status badges

## Troubleshooting

### Scraper Fails

Common issues:
- Invalid or expired tokens
- Rate limit exceeded
- Network timeouts
- Invalid data format

### Missing Data

If data is missing:
- Check scraper logs
- Verify API access
- Test with smaller date range
- Validate output files

### Duplicate Activities

If you see duplicates:
- Check ID generation
- Verify deduplication logic
- Review merge strategy
- Clean up existing data

## Next Steps

- [Deploy your leaderboard](/docs/deployment)
- [Set up automation](/docs/deployment#continuous-deployment)
- [Monitor data quality](/docs/data-management)

